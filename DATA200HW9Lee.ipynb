{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwDQ_z03Q0KB"
   },
   "source": [
    "In this homework, you will be challenged to generate a multivariate logistic regression model using both Statsmodel and Scikit-learn. You should use the cancer.csv dataset. The field 'diagnosis' is a binary field with a 1 indicating a malignant cancer diagnosis and a 0 indicating a benign diagnosis. The columns contain characteristics of the tumor.\n",
    "\n",
    "The data has already been cleaned, you should not need to remove duplicates, nulls, or outliers. Your target variable is your diagnosis. The ID column can be removed. All other columns can be used in your prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQkBP8OMUhtN"
   },
   "source": [
    "Your notebook should show all the code you used to run both models, along with appropriate comments. We should be able to \"Run All\" cells and generate the output you show when turning in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvTyQUf_VqZ3"
   },
   "source": [
    "You will also need to include at least one metric measuring the effectiveness of your models. Compare the metric generated between models. Does one model produce a higher/lower value?\n",
    "\n",
    "Finally, it is often important when working with models to understand which feature is most important in the overall model or contributes to its overall success. Statsmodels generates a list of the p-values for each feature with the summary module. Please run this module on your model and interpret what this means. For sciki-learn, you'll need to work a little harder. There are a number of modules to determine feature importance. One common way is to caculate the coefficients. Please use at least one method to measure feature importance for scikit-learn\n",
    "\n",
    "Your final cell in the workbook should include a write-up of the results of your analysis. Do you think one model is better? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QqOc5V78Udii",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "4          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  fractal_dimension_mean  \n",
       "0         0.2419                 0.07871  \n",
       "1         0.1812                 0.05667  \n",
       "2         0.2069                 0.05999  \n",
       "3         0.2597                 0.09744  \n",
       "4         0.1809                 0.05883  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('cancer.csv')\n",
    "df = df.drop('id', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D42tFmn7V69p",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.372583</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096336</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181233</td>\n",
       "      <td>0.062792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.483918</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.013980</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027331</td>\n",
       "      <td>0.007013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086410</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>0.057800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.061660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195600</td>\n",
       "      <td>0.066080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        diagnosis  radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count  569.000000   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     0.372583    14.127292     19.289649       91.969033   654.889104   \n",
       "std      0.483918     3.524049      4.301036       24.298981   351.914129   \n",
       "min      0.000000     6.981000      9.710000       43.790000   143.500000   \n",
       "25%      0.000000    11.700000     16.170000       75.170000   420.300000   \n",
       "50%      0.000000    13.370000     18.840000       86.240000   551.100000   \n",
       "75%      1.000000    15.780000     21.800000      104.100000   782.700000   \n",
       "max      1.000000    28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096336          0.104341        0.088799             0.048919   \n",
       "std           0.013980          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086410          0.064920        0.029560             0.020310   \n",
       "50%           0.095920          0.092630        0.061540             0.033500   \n",
       "75%           0.105100          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  fractal_dimension_mean  \n",
       "count     569.000000              569.000000  \n",
       "mean        0.181233                0.062792  \n",
       "std         0.027331                0.007013  \n",
       "min         0.106000                0.049960  \n",
       "25%         0.162000                0.057800  \n",
       "50%         0.179400                0.061660  \n",
       "75%         0.195600                0.066080  \n",
       "max         0.304000                0.097440  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodel Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.113948\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:              diagnosis   No. Observations:                  455\n",
      "Model:                          Logit   Df Residuals:                      445\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Tue, 07 Nov 2023   Pseudo R-squ.:                  0.8287\n",
      "Time:                        15:45:07   Log-Likelihood:                -51.847\n",
      "converged:                       True   LL-Null:                       -302.68\n",
      "Covariance Type:            nonrobust   LLR p-value:                2.522e-102\n",
      "==========================================================================================\n",
      "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "radius_mean               -2.3255      3.904     -0.596      0.551      -9.976       5.325\n",
      "texture_mean               0.4177      0.079      5.299      0.000       0.263       0.572\n",
      "perimeter_mean            -0.2830      0.579     -0.489      0.625      -1.418       0.852\n",
      "area_mean                  0.0624      0.013      4.951      0.000       0.038       0.087\n",
      "smoothness_mean           87.4473     38.152      2.292      0.022      12.670     162.224\n",
      "compactness_mean           7.4882     21.150      0.354      0.723     -33.966      48.942\n",
      "concavity_mean             6.3901      8.260      0.774      0.439      -9.800      22.580\n",
      "concave points_mean       62.8285     31.183      2.015      0.044       1.712     123.945\n",
      "symmetry_mean             13.1783     14.168      0.930      0.352     -14.590      40.947\n",
      "fractal_dimension_mean   -65.4663     77.033     -0.850      0.395    -216.447      85.515\n",
      "==========================================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.23 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=200)\n",
    "\n",
    "log_reg = sm.Logit(y_train, X_train).fit() \n",
    "\n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[72  4]\n",
      " [ 5 33]]\n",
      "Test accuracy =  0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "y_hat = log_reg.predict(X_test) \n",
    "prediction = list(map(round, y_hat)) \n",
    "  \n",
    "\n",
    "\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score) \n",
    "  \n",
    "# confusion matrix \n",
    "cm = confusion_matrix(y_test, prediction)  \n",
    "print (\"Confusion Matrix : \\n\", cm)  \n",
    "  \n",
    "# accuracy score of the model \n",
    "print('Test accuracy = ', accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sikit-Learn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8859649122807017\n",
      "Confusion Matrix:\n",
      "[[69  7]\n",
      " [ 6 32]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91        76\n",
      "           1       0.82      0.84      0.83        38\n",
      "\n",
      "    accuracy                           0.89       114\n",
      "   macro avg       0.87      0.88      0.87       114\n",
      "weighted avg       0.89      0.89      0.89       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df['diagnosis']  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=200)\n",
    "\n",
    "model_sklearn = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_sklearn.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_report_sklearn = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{classification_report_sklearn}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.11403508771929824\n",
      "MSE: 0.11403508771929824\n",
      "RMSE: 0.33769081675298523\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "\n",
    "mae = mean_absolute_error(y_true=y_test,y_pred=y_pred)\n",
    "#squared True returns MSE value, False returns RMSE value.\n",
    "mse = mean_squared_error(y_true=y_test,y_pred=y_pred) #default=True\n",
    "rmse = mean_squared_error(y_true=y_test,y_pred=y_pred,squared=False)\n",
    "\n",
    "print(\"MAE:\",mae)\n",
    "print(\"MSE:\",mse)\n",
    "print(\"RMSE:\",rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "radius_mean: -2.2294245812617333\n",
      "texture_mean: 0.2302112885938188\n",
      "perimeter_mean: 0.6073959798312047\n",
      "area_mean: -0.007896815043979055\n",
      "smoothness_mean: 0.4775944329473782\n",
      "compactness_mean: 0.7061101986174045\n",
      "concavity_mean: 1.0226064314833725\n",
      "concave points_mean: 0.6629728800815549\n",
      "symmetry_mean: 0.49938818961350684\n",
      "fractal_dimension_mean: 0.13095275953468202\n"
     ]
    }
   ],
   "source": [
    "coefficients = model_sklearn.coef_\n",
    "\n",
    "# Display the coefficients for each feature\n",
    "feature_names = X_train.columns\n",
    "coefficients_dict = dict(zip(feature_names, coefficients[0]))\n",
    "\n",
    "# Display the coefficients\n",
    "for feature, coef in coefficients_dict.items():\n",
    "    print(f'{feature}: {coef}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, by comparing metrics generated by statsmodel and sikit learn, we were able to observe that the statsmodel logistic regression model's metrics had higer value compare to the sikit learn model.\n",
    "\n",
    "In Statsmodel Logistic Regression, we could observe that the features with high p-values (e.g., 'radius_mean', 'perimeter_mean', 'compactness_mean', 'symmetry_mean', 'fractal_dimension_mean') may not be statistically significant in predicting the target variable and could be considered for removal.  In the case of sikit-learn logistic regression model, a decrease in 'radius_mean' is associated with a decrease in the log-odds of the target being malignant, and the increase of concavity_mean is associated the most with an increase in the log-odds of the target being malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, by comparing the accuracy of the test sample, we could conclude that the statsmodel logistic regression showed better performance with higher accuracy compare the the sikit-learn logistic regression model. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
